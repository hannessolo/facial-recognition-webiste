<h3>Introduction to Neural Networks</h3>
<h4>Introduction by example</h4>
<p>Neural networks are a way to approximate an function. For now, think of a neural network as a black box, with a single input and a single output. In it's initial state, any given input will produce an unpredictable and seemingly random output. The trick is that every time we give it an input, we can also tell the network how close to the desirable output the actual output is. Based on this information, the network will adjust it's inner workings to try and produce a more accurate result. </p>
<p>Suppose we wanted to get our black box to learn the function `x^2`: That is, for every input `x` we give it, we would like it to output the square of the input. We start with a training phase. In this phase, we feed into the network a set of values for which we know the result we would like it to output. This is called the training data. In our case, this is easy to obtain, as we know how to calculate `x^2`. This approach to machine learning is called supervised learning, as we watch the network learn and essentially teach it. <a href="/PCA">PCA</a> is an example of unsupervised learning.</p>
<img class='text-image' src='public/img/cnn_fig_1.svg' />
<div class='image-caption'>
  &lt;Fig.1: A "black box" approximating the function `x^2`.&gt;
  <a>Source: own content</a>
</div>
<p>At this point we are ready to feed our training data into the network. We iteratively give it an input, wait for the output, then tell it how close it came. There are many measures of closeness, but for this simple example, the absolute difference between the output and expected output is good enough.</p>
<p>After all training data has been fed into the network, we can move on to actually using the network. We now feed the black box values it has never seen before, and hope that it can output a value close to what we would have hoped based on the patterns it has observed. </p>
<p>Now, this example is obviously useless: If there is one thing computers are good at, it's calculating things, and squaring numbers is a very easily solved problem. Where they excel is if the inputs and outputs are known, however not the actual function that will get us from the input to the output.</p>
<p>Take another example: We would like our network to output a 1 if we input a picture of a cat, and a 0 for a picture of a dog. There is no known mathematical function that will take us from pictures of dogs or cats to 1s and 0s - but through deep learning, our network can approximate one. </p>
<p>So we do the same as before, feed it a bunch of images of cats and dogs (as a matrix of pixel values) and tell it which was which. Note, however, that the training data is much harder to obtain here. For `x^2`, it is easy to generate the first `1000` squares, and use those as training data. But with pictures, we must label them manually. This is often the hardest part of deep learning.</p>
<h4>Inside the Black Box</h4>
<p>At this point, you may be curious how this black box actually works. The simplest form of a neural network is a multilayer perceptron. It is made up of several layers of interconnected "neurons". Each layer has a number of neurons. But not every layer must have the same number of neurons. Each arc in the graph has associated with it a weight. All neurons are connected to all neurons of the previous layer, as well as all neurons of  the next layer, except the left and right most layers. The leftmost layer is called the input layer, as this is where we input the values into the network. The rightmost layer is called the output layer, as this is where we read the output of the network. For our `x^2` example, the input and output layers would both be a single neuron, as the function takes a single argument and outputs a single value. The layers between the input and output are called hidden layers, as these are what we called the "black box". </p>
<img class='text-image' src='public/img/cnn_fig_2.png' />
<div class='image-caption'>
  &lt;Fig.2: Representation of a neural network. The circles represent induvidual neurons. Layers are veritcal segments. Arrows are wheighted connections (inputs and outputs) between neurons.&gt;
  <a href='http://neuralnetworksanddeeplearning.com/images/tikz11.png'>Source: neuralnetworksanddeeplearning.com</a>
</div>
<p>So what does a neuron do? Its actually a lot simpler than you might think. It simply sums its inputs, puts it thorough a non-linear function (often a logistic function), and outputs that value. That's all a neuron does. The following equation describes the output for a neuron that has k inputs, where `g` is the logistic function, and `A[k]` is the `k^{th}` input:</p>
<p class="equation">`g(sum_(n=0)^k A[n])`</p>
<p>
  The way this neuron functions is inspired by biological neurons in brains. Induvidually,
  they simply take their inputs (sum them) and then decide wether to output something
  or not (non-linear activation function). But collectively, they can learn complex behaviors.
</p>
<p>But now, how does this network learn? Through a process called gradient descent. We need to find some measure of how well the network is doing, and call this the loss function. This is often the mean squared error. We want to optimize the function on this value - that is, get the value as low as possible.</p>
<img src='public/img/cnn_fig_3.png' class='text-image' />
<div class='image-caption'>
  &lt;Fig.3: Exampe of gradient descent. This image shows the plot of an loss function `J` depending on only 2 weights, `\theta_0` and `\theta_1`. The black lines show steps in the process of gradient descent, having started at different initial weight values.&gt;
  <a href='https://cdn-images-1.medium.com/max/703/1*t4aYsxpCqz2eymJ4zkUS9Q.png'>Source: medium.com</a>
</div>
<p>To do this, for each of the weights that lead to the output layer, we take the partial derivative of the error function with respect to that weight. The derivative tells us in which direction the value should change to get a better result. Think of this as descending down a mountain - you look around (take the derivative) and then take a step in the direction that is the steepest. How big that step is, we set using the learning rate - an arbitrary number that is decided through trial and error, but usually stays constant throughout training. We then repeat this for every layer, until we reach the input layer. We hope that the network is now slightly more accurate, enter the next training data point, and repeat the algorithm.</p>
<h3>Convolutional Nerual Networks</h3>
<p>For this article, we will consider convolutional nerual networks (CNNs), however other architectures also exist. We choose CNNs because they are the most prevalent in image recognition - which is what facial recognition is interested in.</p>
<p>Convolutional neural networks differ from standard multilayer perceptrons in the way the neurons are connected, and in the data is entered. When we enter an image into a standard network, we have to flatten it. The layers are all one dimensional, like a vector. We take an image, which is 2 dimensional, and somehow have to squash it into a vector. This way, valuable information, like the notion of proximity, is lost.</p>
<p>The answer to this problem is convolutional neural networks. CNNs change the way we have to think of neurons. Neurons now have a grid of inputs, each of the squares in the grid associated with a weight. The grid projects its weighted values onto a neuron corresponding to the grids current position. The grid then moves. Now the process works exactly the same as before: Every neuron sums all its weighted inputs and puts them through an output function. However, this time we keep the 2 dimensional structure of the image intact in both the previous and current layer. This means that this method of convolution can be applied in multiple layers. Notice that not now, not every neuron (=item in the grid) is connected to every neuron of the next layer.</p>
<img src='public/img/cnn_fig_4.png' class='text-image' />
<div class='image-caption'>
  &lt;Fig.4: Simplified representation of 2 layers in a convolutional neural netowrk. Usually, each layer would include several images ("channels"). We would apply grids with different weights to the input images, wich means we get the number of input channels times the number of different grids output channels.&gt;
  <a>Source: own content</a>
</div>
<p>For the output layer, we often use what is called a fully connected layer: This means we use a layer as in a multilayer perceptron, where every neuron is connected to every neuron in the next layer. The number of neurons in the output layer can vary depending on the chosen representation. See the next section. </p>
<h3>Neural Networks Applied for Facial Recognition</h3>
<h4>Dataset Collection</h4>
<p>
  Neural Networks have not been applied to facial recognition until recently in part because it is hard to obtain a large enough data set. Even now, such technology is mostly restricted to large companies such as Facebook or Google, that collect these data sets by nature of their business. To train a network effectively, hundreds of thousands to millions of images of thousands of individuals are required. Some state of the art facial recognition algorithms by Google were even trained on over 200 million images.
So how are data sets of this magnitude constructed? There is no way to label all of the data by hand. So we must do better.
</p>
<p>
  A good place to start is some sort of database, like IMDB (Internet Movie Database), that is freely available. From this sort of database, it is often possible to get a few thousand identities from actors, who might have, on average, 3 pictures uploaded.
</p>
<p>
  We then choose a sub set of this database as our candidate data set. Here, we need to make sure that our data set is as balanced as possible: Around half of the identities should be male, the other half female. We also need to make sure different ethnicities are represented proportionally. Otherwise, our network could struggle after the training phase.
</p>
<p>
  As a next step, we need to find more images for each identity in our candidate list. This can be done for instance by using google image search and downloading the top 200 results for each name. However, among these images are bound to be images that are not usable, so they must be filtered. This can be done in part manually, in part using other facial recognition algorithms, such as Eigenfaces, to rank the images by usability. A usable image must be reasonably frontal, and not include too many other things.
</p>
<p>
  In the end, we filter out any of the identities that went over a threshold of unusable images: For instance, if more than 10 percent of images was not usable, drop this identity. For the identities that remain, we also need to filter out near duplicate images, as this could lead to over fitting, a common problem of machine learning, whereby an algorithm learns too many specifics of one image and can't generalize anymore.
</p>
<h4>Network Architecture</h4>
<p>
  We will now consider a specific architecture of neural network for facial recognition proposed by Parkhi et al. in 2015.
</p>
<p>
  This architecture is made up of 2 parts. In the first part, a CNN is trained as in a N-ways classification problem, meaning it can only classify any given image as one of N identities given in the training data. They use a CNN. The output layer is a fully connected layer and uses a one-hot encoding. This means the output layer is a N-dimensional vector. Given image number `n \in [0..N]`, we want the network to output a vector where all values are close to 0, except for `N_n`, which we want to be close to 1. This means each item of the vector corresponds to the certainty of the network that the input was face `n`. This first CNN is trained in a relatively straight forward way, similar to the methods described in previous sections.
</p>
<p>
  The last layer of the neural network is removed - this means that the one-hot encoding and classification into `N` identities is no longer there. We make the second to last layer the new output layer. What we are left with is an abstract encoding for images. This encoding is still simply a high dimensional vector. We can now input different faces, and verify identities by comparing the euclidean distance between the output vectors - the distance should be high for different identities, and small for the same identity.
</p>
<p>
  However, to further improve results, a second stage of learning, called triplet based learning, is conducted. In triplet based learning, instead of using a conventional loss function, we use sets of 3 images. One of these faces is called pivot, and is used as a frame of reference. The next image is an image of the same identity as the pivot, the third image is an image of a different identity. The goal of training is to adjust the weights in the network such that the distance between the pivot and the same identity is as small as possible, while also making the distance between the pivot and the negative face as large as possible.
</p>
<img src='public/img/cnn_fig_5.png' class='text-image' />
<div class='image-caption'>
  &lt;Fig.5: Triplet learning: Fictional representation images of 2 US presidents in 3D space, as it could apprear from a CNN. The images are the input, the vectors the output. In practice, much higher dimensional space would be used.&gt;
  <a>Source: own content</a>
</div>
<p>
  Using these techniques in conjunction with each other, Parki et al. were able to achieve an accuracy of up to 99.13 percent on benchmark data sets. This is much higher than non-deep learning methods, which often only yield around 90 to 95 per cent accuracy. This would explain the rise of deep learning, and the decline of other machine learning, in the last few years: Deep learning has beat all the other methods in terms of accuracy.
</p>
