<h3>Introduction to Neural Networks</h3>
<h4>Introduction by example</h4>
<p>Neural networks are a way to approximate an function. For now, think of a neural network as a black box, with a single input and a single output. In it's initial state, any given input will produce an unpredictable and seemingly random output. The trick is that every time we give it an input, we can also tell the network how close to the desirable output the actual output is. Based on this information, the network will adjust it's inner workings to try and produce a more accurate result. </p>
<p>Suppose we wanted to get our black box to learn the function `x^2`: That is, for every input `x` we give it, we would like it to output the square of the input. We start with a training phase. In this phase, we feed into the network a set of values for which we know the result we would like it to output. This is called the training data. In our case, this is easy to obtain, as we know how to calculate `x^2`. This approach to machine learning is called supervised learning, as we watch the network learn and essentially teach it. <a href="/PCA">PCA</a> is an example of unsupervised learning.</p>
<p>At this point we are ready to feed our training data into the network. We iteratively give it an input, wait for the output, then tell it how close it came. There are many measures of closeness, but for this simple example, the difference between the output and expected output is good enough.</p>
<p>After all training data has been fed into the network, we can move on to actually using the network. We now feed the black box values it has never seen before, and hope that it can output a value close to what we would have hoped based on the patterns it has observed. </p>
<p>Now, this example is obviously useless: If there is one thing computers are good at, it's calculating things, and squaring numbers is a very easily solved problem. Where they excel is if the inputs and outputs are known, however not the actual function that will get us from the input to the output.</p>
<p>Take another example: We would like our network to output a 1 if we input a picture of a cat, and a 0 for a picture of a dog. There is no known mathematical function that will take us from pictures of dogs or cats to 1s and 0s - but through deep learning, our network can approximate one. </p>
<p>So we do the same as before, feed it a bunch of images of cats and dogs (as a matrix of pixel values) and tell it which was which. Note, however, that the training data is much harder to obtain here. For `x^2`, it is easy to generate the first `1000` squares, and use those as training data. But with pictures, we must label them manually. This is often the hardest part of deep learning.</p>
<h4>Inside the Black Box</h4>
<p>At this point, you may be curious how this black box actually works. The simplest form of a neural network is a multilayer perceptron. It is made up of several layers of interconnected "neurons". Each layer has a number of neurons. But not every layer must have the same number of neurons. Each arc in the graph has associated with it a weight. All neurons are connected to all neurons of the previous layer, as well as all neurons of  the next layer, except the left and right most layers. The leftmost layer is called the input layer, as this is where we input the values into the network. The rightmost layer is called the output layer, as this is where we read the output of the network. For our `x^2` example, the input and output layers would both be a single neuron, as the function takes a single argument and outputs a single value. The layers between the input and output are called hidden layers, as these are what we called the "black box". </p>
<p>So what does a neuron do? Its actually a lot simpler than you might think. It simply sums its inputs, puts it thorough a non-linear function (often a logistic function), and outputs that value. That's all a neuron does. The following equation describes the output for a neuron that has k inputs, where `g` is the logistic function, and `A[k]` is the `k^{th}` input:</p>
<p class="equation">`g(sum_(n=0)^k A[n])`</p>
<p>But now, how does this network learn? Through a process called gradient descent. We need to find some measure of how well the network is doing, and call this the loss function. This is often the mean squared error. We want to optimize the function on this value - that is, get the value as low as possible.</p>
<p>To do this, for each of the weights that lead to the output layer, we take the partial derivative of the error function with respect to that weight. The derivative tells us in which direction the value should change to get a better result. Think of this as descending down a mountain - you look around (take the derivative) and then take a step in the direction that is the steepest. How big that step is, we set using the learning rate - an arbitrary number that is decided through trial and error, but usually stays constant throughout training. We then repeat this for every layer, until we reach the input layer. We hope that the network is now slightly more accurate, enter the next training data point, and repeat the algorithm.</p>
<h3>Convolutional Nerual Networks</h3>
<p>For this article, we will consider convolutional nerual networks (CNNs), however other architectures also exist. We choose CNNs because they are the most prevalent in image recognition - which is what facial recognition is interested in.</p>
<p>Convolutional neural networks differ from standard multilayer perceptrons in the way the neurons are connected, and the data is entered. When we enter an image into a standard network, we have to flatten it. The layers are all one dimensional, like a vector. We take an image, which is 2 dimensional, and somehow have to squash it into a vector. This way, valuable information, like the notion of proximity, is lost.</p>
<p>The answer to this problem is convolutional neural networks. CNNs change the way we have to think of neurons. Neurons now have a grid of inputs, each of the square in the grid associated with a weight. Each neuron projects this grid onto a specific location of the input. It then works exactly the same as before: It sums all its weighted inputs and puts them through an output function. However, this time we keep the 2 dimensional structure of the image intact in both the previous and current layer. This means that this method of convolution can be applied in multiple layers. Notice that not now not every neuron is connected to every neuron of the next layer. Often, it is also applied multiple times per layer. </p>
<p>For the output layer, we often use what is called a fully connected layer: This means we use a layer as in a multilayer perceptron, where every neuron is connected to every neuron in the next layer. The number of neurons in the output layer can vary depending on the chosen representation. See the next section. </p>
<h3>Neural Networks Applied for Facial Recognition</h3>